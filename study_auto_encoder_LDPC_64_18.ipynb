{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Belief Propagation Auto-Encoder - AE (64,18) Study\n",
    "The proposed architecture is configured to learned (64,18) codes and compared with conventional LDPC codes designed using the PEG method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "from platform import python_version\n",
    "print(f'python: {python_version()}')\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f'tensorflow: {tf.version.VERSION}')\n",
    "\n",
    "physical_devices_available = tf.config.list_physical_devices()\n",
    "print(f'Available physical devices: {physical_devices_available}')\n",
    "\n",
    "import os\n",
    "os.environ['TF_GPU_THREAD_MODE']='gpu_private'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Get the code G and H matrices\n",
    "(n,k)=(64,18)\n",
    "codename = f'LDPC/n{n}_k{k}_m46_dv2-4_dc3-4'\n",
    "#! codename = f'hamming_{n}_{k}'\n",
    "code_path = os.path.join('./','encoders/linearblockencoders_reference/', f'{codename}.npz')\n",
    "\n",
    "code_file = np.load(code_path)\n",
    "# print(code_file.files)\n",
    "\n",
    "\n",
    "G_sys = tf.convert_to_tensor(code_file['G_systematic'], dtype=tf.float32)\n",
    "G_nsys = tf.convert_to_tensor(code_file['G_non_systematic'], dtype=tf.float32)\n",
    "H_sys = tf.convert_to_tensor(code_file['H_systematic'], dtype=tf.float32)\n",
    "H_nsys = tf.convert_to_tensor(code_file['H_non_systematic'], dtype=tf.float32)\n",
    "\n",
    "tf.print(G_sys,summarize=-1)\n",
    "tf.print(G_nsys,summarize=-1)\n",
    "tf.print(H_sys,summarize=-1)\n",
    "tf.print(H_nsys,summarize=-1)\n",
    "tf.print(tf.matmul(G_sys,tf.transpose(H_sys))%2,summarize=-1)\n",
    "tf.print(tf.matmul(G_sys,tf.transpose(H_nsys))%2,summarize=-1)\n",
    "tf.print(tf.matmul(G_nsys,tf.transpose(H_sys))%2,summarize=-1)\n",
    "tf.print(tf.matmul(G_nsys,tf.transpose(H_nsys))%2,summarize=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import random_messages_dataset, random_messages_base_all_zero_all_one_dataset #!, random_messages_base_dataset, all_zero_dataset,\n",
    "\"\"\"\n",
    "#Dataset logic:\n",
    ".take() allow to generate a fixed number of batch of the dataset\n",
    ".cache() allow to use cached data (i.e doesn't execute previous dataset generation instructions)\n",
    ".shuffle() suffle elements inside batches by picking randomly elmts in a buffer of size buffer_size. similarly to batching procedure, shuffling method of model.fit isn't called when using dataset. it's important to manualy specify a shuffling strategy then.\n",
    ".prefetch() is used to overlap the processing time of the data producer and data consumer. should be used as last step. \n",
    "Here the step before .cache() should be done once and for all at first call of the train_dataset object. The operations after .cache() should be called at each time the dataset is called (ie for each batch in the training).\n",
    "All the steps before .prefetch() are supposed to be prepared during the training step of the data consumer to be ready when the consumer needs more data.\n",
    "\"\"\"\n",
    "\n",
    "#TRAIN\n",
    "epochs=1000\n",
    "steps_per_epoch=25\n",
    "train_batch = 64\n",
    "train_seed = 42\n",
    "#train_dataset = random_messages_dataset(k, batch=64, prefetch=tf.data.AUTOTUNE, seed=train_seed)\n",
    "#train_dataset = random_messages_base_dataset(k, batch=64, prefetch=tf.data.AUTOTUNE, seed=train_seed)\n",
    "#train_dataset = all_zero_dataset(k, batch=64, prefetch=tf.data.AUTOTUNE)\n",
    "#train_dataset = random_messages_base_all_zero_all_one_dataset(k, batch=train_batch, prefetch=tf.data.AUTOTUNE, seed=train_seed)#.take(steps_per_epoch*epochs).cache().prefetch(tf.data.AUTOTUNE)#.shuffle(buffer_size=train_batch*steps_per_epoch)\n",
    "train_dataset = random_messages_base_all_zero_all_one_dataset(k, batch=train_batch, prefetch=tf.data.AUTOTUNE, seed=train_seed).take(steps_per_epoch * epochs).cache()\n",
    "\n",
    "#VALIDATION\n",
    "validation_seed = 43\n",
    "val_batch_size = 64\n",
    "validation_dataset = random_messages_dataset(k, batch=val_batch_size, prefetch=tf.data.AUTOTUNE, seed=validation_seed).take(int(8000 / val_batch_size)).cache()\n",
    "\n",
    "#TEST\n",
    "#!N_Bytes = 20000 * k\n",
    "#!N_bits = N_Bytes * 8\n",
    "#!N_words = N_bits / k\n",
    "test_batch_size = 100  #!\n",
    "test_seed = 44\n",
    "test_dataset = random_messages_dataset(\n",
    "        k,\n",
    "        batch=test_batch_size,  #!\n",
    "        prefetch=tf.data.AUTOTUNE,\n",
    "        seed=test_seed,  #!\n",
    "    )#!.take(int(N_words / test_batch_size)).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import ebno_db_to_snr_db\n",
    "# training and evaluation setup\n",
    "# \n",
    "#ebn0_training_dbs = tf.range(0, 9, delta=1, dtype=tf.float32)\n",
    "ebn0_training_dbs = 4\n",
    "ebn0_eval_dbs = tf.range(0, 7, delta=1, dtype=tf.float32)\n",
    "noise_power_training_dbs = -ebno_db_to_snr_db(ebn0_training_dbs, k/n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import create_paths_and_summaries\n",
    "\n",
    "paths_and_summaries = create_paths_and_summaries('study-ae-paper\\\\study-ae-{n}-{k}'.format(n=n,k=k), 'Eb/N0 (dB)', ebn0_eval_dbs)\n",
    "summary_ber = paths_and_summaries.summary_ber\n",
    "summary_bler = paths_and_summaries.summary_bler\n",
    "summary_bec = paths_and_summaries.summary_bec\n",
    "summary_blec = paths_and_summaries.summary_blec\n",
    "summary_bpci_ber = paths_and_summaries.summary_bpci_ber\n",
    "summary_bpci_bler = paths_and_summaries.summary_bpci_bler\n",
    "models_path = paths_and_summaries.models_path\n",
    "tensorboard_path = paths_and_summaries.tensorboard_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "from autoencoders import AutoEncoder\n",
    "from metrics import BitErrorRate, BlockErrorRate, BinomialProportionConfidenceInterval, BitErrorCount, BlockErrorCount\n",
    "\n",
    "noise_power_training_dbs = -ebno_db_to_snr_db(ebn0_training_dbs, k/n)\n",
    "\n",
    "def create_model(\n",
    "    n,\n",
    "    k,\n",
    "    build_dataset,  #! Used to build model graph at creation\n",
    "    conf=\"A\",\n",
    "    learning_rate=1e-1,\n",
    "    model_index=None,\n",
    "    training_noise_power_db=[0.0],\n",
    "    train_model=True,\n",
    "    G=None,\n",
    "    H=None,\n",
    "    trainable_code=True,\n",
    "    trainable_decoder=True,\n",
    "    additional_confs=[],\n",
    "    name=None,\n",
    "):\n",
    "    if name is None:\n",
    "        conf_string = str(conf) if conf is not None else \"\"\n",
    "        lr_string = str(learning_rate) if learning_rate is not None else \"\"\n",
    "        index_string = str(model_index) if model_index is not None else \"\"\n",
    "        train_model_string = str(train_model) if train_model is not None else \"\"\n",
    "        trainable_code_string = (\n",
    "            str(trainable_code) if trainable_code is not None else \"\"\n",
    "        )\n",
    "        name = f\"AE-{n}-{k}-{conf_string}-{index_string}\"  #!-{lr_string} float number harmful because of the decimal dot in file name eg 0.01\n",
    "    else:\n",
    "        name = name\n",
    "\n",
    "    n_iter = 5\n",
    "    model = AutoEncoder(\n",
    "        n,\n",
    "        k,\n",
    "        n_iter,\n",
    "        conf,\n",
    "        training_noise_power_db=training_noise_power_db,\n",
    "        G=G,\n",
    "        H=H,\n",
    "        trainable_code=trainable_code,\n",
    "        trainable_decoder=trainable_decoder,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    metric_list = [\n",
    "            BitErrorRate(name=\"BER\",from_logits=False),\n",
    "            BlockErrorRate(name=\"BLER\",from_logits=False),\n",
    "            BitErrorCount(name=\"BEC\",from_logits=False, mode=\"sum\"), \n",
    "            BlockErrorCount(name=\"BLEC\",from_logits=False, mode=\"sum\"), \n",
    "            BinomialProportionConfidenceInterval(monitor_class=BitErrorRate, monitor_params={\"name\":\"bpci_ber\",\"from_logits\":False},fraction=0.95, name=\"BPCI_BER\"),\n",
    "            BinomialProportionConfidenceInterval(monitor_class=BlockErrorRate, monitor_params={\"name\":\"bpci_bler\",\"from_logits\":False},fraction=0.95, name=\"BPCI_BLER\")\n",
    "        ]\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=metric_list,\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Build model graph using build dataset\n",
    "    build_datum = list(build_dataset.take(1))  #!\n",
    "    model(build_datum[0][0])  #!\n",
    "    # model(np.array([train_dataset[0]]))\n",
    "    # tf.print(model.get_weights())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks.defaults import default_training_callbacks, default_configuration_early_stopping, default_configuration_tensorboard, default_configuration_reduce_lr_on_plateau, default_configuration_model_checkpoint\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    models_path,\n",
    "    train_dataset,\n",
    "    validation_dataset,\n",
    "    tensorboard_path,\n",
    "    epochs=1000,\n",
    "    steps_per_epoch=25,\n",
    "):\n",
    "\n",
    "    ckpt_path = os.path.join(models_path, \"checkpoint\\checkpoint.tf\")\n",
    "    configuration_tensorboard = default_configuration_tensorboard(tensorboard_path)\n",
    "    configuration_earlystopping = default_configuration_early_stopping(\n",
    "        monitor=\"loss\", patience=200\n",
    "    )\n",
    "    configuration_reduce_lr_on_plateau = default_configuration_reduce_lr_on_plateau(\n",
    "        monitor=\"val_loss\", factor=0.8, patience=50\n",
    "    )\n",
    "    configuration_model_checkpoint = default_configuration_model_checkpoint(\n",
    "        filepath=ckpt_path\n",
    "    ) \n",
    "    callbacks = default_training_callbacks(\n",
    "        configuration_earlystopping=configuration_earlystopping,\n",
    "        configuration_tensorboard=configuration_tensorboard,\n",
    "        configuration_reduce_lr_on_plateau=configuration_reduce_lr_on_plateau,\n",
    "        configuration_model_checkpoint=configuration_model_checkpoint,\n",
    "    )\n",
    "\n",
    "    # if needed\n",
    "    # from dataset import random_messages\n",
    "    # validation_messages = random_messages(k, 10_000)\n",
    "    # bit_error_distribution_callback = BitErrorDistribution(tensorboard_path, eval_dataset=validation_messages, from_logits=False)\n",
    "    # callbacks.append(bit_error_distribution_callback)\n",
    "\n",
    "    # latent_space_callback = LatentSpaceDistribution(tensorboard_path, eval_dataset=validation_messages)\n",
    "    # callbacks.append(latent_space_callback)\n",
    "\n",
    "    # ds = np.array(list(train_dataset))\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        model.fit(\n",
    "            x=train_dataset,  # tf.reshape(ds[:,0],shape=(50*64,16)),#\n",
    "            validation_data=validation_dataset,\n",
    "            # y=train_dataset,#y=tf.reshape(ds[:,1],shape=(50*64,16)),\n",
    "            epochs=epochs,\n",
    "            # batch_size=train_batch,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "        )\n",
    "    model.load_weights(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import BatchTerminationCallback\n",
    "# model evaluation\n",
    "#import os\n",
    "\n",
    "def ber_ci_condition(_, logs):\n",
    "    if 'BPCI_BER' in logs:\n",
    "        epsilon = 1e-7\n",
    "        (ci_span, ci_low, ber, ci_high) = logs['BPCI_BER']\n",
    "        return (ci_span)/(ber+epsilon) < 0.1\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def bler_ci_condition(_, logs):\n",
    "    if 'BPCI_BLER' in logs:\n",
    "        epsilon = 1e-7\n",
    "        (ci_span, ci_low, bler, ci_high) = logs['BPCI_BLER']\n",
    "        return (ci_span)/(bler+epsilon) < 0.1\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def evaluate_model(summary_ber, summary_bler, summary_bec, summary_blec, summary_bpci_ber, summary_bpci_bler, models_path, model, ebn0_eval_dbs, test_dataset, k, n):\n",
    "    # tf.print(model.get_weights())\n",
    "    bers = []\n",
    "    blers = []\n",
    "    becs = []  \n",
    "    blecs = []\n",
    "    bpci_bers = [] \n",
    "    bpci_blers = []\n",
    "    \n",
    "    # mean(Es) assumed to be 1\n",
    "    noise_power_eval_dbs = -ebno_db_to_snr_db(ebn0_eval_dbs, k / n)\n",
    "    snr_eval_dbs = ebno_db_to_snr_db(ebn0_eval_dbs, k / n)\n",
    "\n",
    "    # add SNR values\n",
    "    summary_ber[\"SNR(dB)\"] = snr_eval_dbs.numpy()\n",
    "    summary_bler[\"SNR(dB)\"] = snr_eval_dbs.numpy()\n",
    "    summary_bec[\"SNR(dB)\"] = snr_eval_dbs.numpy()\n",
    "    summary_blec[\"SNR(dB)\"] = snr_eval_dbs.numpy()\n",
    "    summary_bpci_ber[\"SNR(dB)\"] = snr_eval_dbs.numpy()\n",
    "    summary_bpci_bler[\"SNR(dB)\"] = snr_eval_dbs.numpy()\n",
    "\n",
    "    for ebn0_eval_db, noise_power_eval_db in zip(ebn0_eval_dbs, noise_power_eval_dbs):\n",
    "        print(\n",
    "            f\"evaluating {model.name} at Eb/N0 [dB]: {ebn0_eval_db} / N0 [dB]: {noise_power_eval_db}\"\n",
    "        )\n",
    "        model.channel.noise_power_db=noise_power_eval_db\n",
    "\n",
    "        termination_callback = BatchTerminationCallback(ber_ci_condition)\n",
    "        summary = model.evaluate(\n",
    "            test_dataset,  # validation_dataset,\n",
    "            steps=25000,\n",
    "            return_dict=True,\n",
    "            callbacks=[termination_callback]\n",
    "        )\n",
    "        \n",
    "        (ber_ci_span, ci_min, ber_bpci_metric, ci_max) = summary['BPCI_BER']\n",
    "        (bler_ci_span, ci_min, bler_bpci_metric, ci_max) = summary['BPCI_BLER']\n",
    "        \n",
    "        #print(summary)\n",
    "        ber = summary[\"BER\"]  \n",
    "        bler = summary[\"BLER\"] \n",
    "        bec = summary[\"BEC\"]  \n",
    "        blec = summary[\"BLEC\"] \n",
    "        bpci_ber = summary[\"BPCI_BER\"]  \n",
    "        bpci_bler = summary[\"BPCI_BLER\"] \n",
    "        bers.append(ber) \n",
    "        blers.append(bler)\n",
    "        becs.append(bec) \n",
    "        blecs.append(blec)\n",
    "        bpci_bers.append(bpci_ber) \n",
    "        bpci_blers.append(bpci_bler)\n",
    "        \n",
    "        print(f\"Eb/N0: {ebn0_eval_db} BER: {ber} 95% CI: {ber_ci_span} - BEC: {bec} - BLER: {ber} 95% CI: {bler_ci_span} - BLEC: {blec}\")\n",
    "\n",
    "    # learned_code\n",
    "    G, H = model.code_generator(None)\n",
    "\n",
    "    # store results and model\n",
    "    summary_ber[model.name] = bers\n",
    "    summary_bler[model.name] = blers\n",
    "    summary_bec[model.name] = becs\n",
    "    summary_blec[model.name] = blecs\n",
    "    summary_bpci_ber[model.name] = bpci_bers\n",
    "    summary_bpci_bler[model.name] = bpci_blers\n",
    "\n",
    "    model_path = os.path.join(models_path, model.name)\n",
    "    encoder_path = os.path.join(model_path, model.encoder.name)\n",
    "    decoder_path = os.path.join(model_path, model.decoder.name)\n",
    "    code_generator_path = os.path.join(model_path, model.code_generator.name)\n",
    "    matrices_path = os.path.join(model_path, \"matrices\")\n",
    "\n",
    "    print(model_path, encoder_path, decoder_path, code_generator_path)\n",
    "    os.makedirs(encoder_path, exist_ok=True)\n",
    "    os.makedirs(decoder_path, exist_ok=True)\n",
    "    os.makedirs(code_generator_path, exist_ok=True)\n",
    "    os.makedirs(matrices_path, exist_ok=True)\n",
    "    print(f\" saving model {model.name} in {model_path}\")\n",
    "    # tf.keras.utils.plot_model(model, to_file=model_path+'/model.png', show_shapes=True, show_dtype=False,show_layer_names=True, rankdir='TB', expand_nested=True, dpi=96)\n",
    "    # model.save(model_path,overwrite=True)\n",
    "    model.encoder.save(encoder_path, overwrite=True)\n",
    "    model.decoder.save(decoder_path, overwrite=True)\n",
    "    model.code_generator.save(code_generator_path, overwrite=True)\n",
    "    np.savetxt(matrices_path + \"\\G.csv\", np.array(G), fmt=\"%i\")\n",
    "    np.savetxt(matrices_path + \"\\H.csv\", np.array(H), fmt=\"%i\")\n",
    "    \n",
    "    #x = list(test_dataset.take(1))[0][0][0:10]\n",
    "    #tf.print(tf.cast(x,dtype=tf.int32),summarize=-1)\n",
    "    #tf.print(tf.cast(model(x),dtype=tf.int32),summarize=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple\n",
    "from tools import configurations_product,configurations_list\n",
    "\n",
    "#print(tf.executing_eagerly())\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "options = ['n','k','build_dataset','conf','learning_rate','model_index','training_noise_power_db','train_model','G','H','trainable_code','trainable_decoder','additional_confs','name']\n",
    "\n",
    "ML_eval_conf = configurations_list(options,[[n,k,train_dataset,'ML',None,0,noise_power_training_dbs,False,None,None,False,False,[],\"ML\"]])[0]\n",
    "BP_eval_conf = configurations_list(options,[[n,k,train_dataset,'BP',None,0,noise_power_training_dbs,False,None,None,False,False,[],\"BP\"]])[0]\n",
    "GNBP_eval_conf = configurations_list(options,[[n,k,train_dataset,'GNBP',1e-1,0,noise_power_training_dbs,True,None,None,False,True,[],\"GNBP\"]])[0]\n",
    "\n",
    "n_trials= 5\n",
    "\n",
    "config_list = [[n,k,train_dataset,'A',1e-1,i,noise_power_training_dbs,True,None,None,True,True,[BP_eval_conf],f'AE_GNBP_{i}'] for i in range(n_trials)]                \\\n",
    "+[[n,k,train_dataset,'BP',None,0,noise_power_training_dbs,False,G_sys,H_sys,False,False,[],'LDPC_SYS_BP']]                                                        \\\n",
    "+[[n,k,train_dataset,'GNBP',1e-1,i,noise_power_training_dbs,True,G_sys,H_sys,False,True,[],f'LDPC_SYS_GNBP_{i}'] for i in range(n_trials)]                       \\\n",
    "+[[n,k,train_dataset,'BP',None,0,noise_power_training_dbs,False,G_sys,H_nsys,False,False,[],'LDPC_NSYS_BP']]                                                       \\\n",
    "+[[n,k,train_dataset,'GNBP',1e-1,i,noise_power_training_dbs,True,G_sys,H_nsys,False,True,[],f'LDPC_NSYS_GNBP_{i}'] for i in range(n_trials)]                        \n",
    "\n",
    "#TODO: improve configuration and default configuration + add n trials as a configuration param\n",
    "#print(config_list)\n",
    "configurations = configurations_list(options,config_list)\n",
    "for c in configurations:\n",
    "    #print(c)\n",
    "    model = create_model(**c._asdict())\n",
    "    if c.train_model:\n",
    "        train_model(model, models_path, train_dataset, validation_dataset, tensorboard_path,epochs,steps_per_epoch)\n",
    "    (G,H) = model.code_generator(None)\n",
    "    tf.assert_equal(tf.math.floormod(tf.matmul(G,tf.transpose(H)),2), tf.zeros(shape=(k,(n-k))),message=\"Generator and PC matrices are not matched as syndrome matrix (G.H^T) is not equal to 0\")\n",
    "    evaluate_model(summary_ber, summary_bler, summary_bec, summary_blec, summary_bpci_ber, summary_bpci_bler, models_path, model, ebn0_eval_dbs, test_dataset, k, n)\n",
    "    \n",
    "    if c.additional_confs != []:\n",
    "        parent_model_name = model.name\n",
    "        for conf in c.additional_confs:           \n",
    "            conf = conf._asdict()\n",
    "            conf['G'] = G\n",
    "            conf['H'] = H\n",
    "            conf['name'] = parent_model_name + \"_\" +conf['name']\n",
    "            Configuration = namedtuple(\"Configuration\", options)\n",
    "            conf = Configuration(**conf)\n",
    "            print(\"Additionnal Conf:\")\n",
    "            model = create_model(**conf._asdict())\n",
    "            if conf.train_model:\n",
    "                train_model(model, models_path, train_dataset, validation_dataset, tensorboard_path, epochs, steps_per_epoch)\n",
    "            evaluate_model(summary_ber, summary_bler, summary_bec, summary_blec, summary_bpci_ber, summary_bpci_bler, models_path, model, ebn0_eval_dbs, test_dataset, k, n)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Orange\n",
    "\n",
    "Authors: Guillaume Larue <guillaume.larue@orange.com>, Quentin Lampin <quentin.lampin@orange.com>, Louis-Adrien Dufrene <louisadrien.dufrene@orange.com>\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), \n",
    "to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, \n",
    "and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice (including the next paragraph) shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS \n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER \n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "838511658caab0ac8ee3679c47d797e08a1e9fafeaab2300cd77b884ea15c881"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
